# VL Benchmarking Suite

This directory contains tools to measure and verify the token efficiency of Vibe Language compared to Python.

## Prerequisites

To run the benchmarks effectively and get accurate token counts (matching GPT-4/Claude), you need the `tiktoken` library.

```bash
pip install tiktoken
```

## Running Benchmarks

Run the benchmark script from the root of the repository:

```bash
python benchmarks/benchmark_suite.py
```

## Methodology

The benchmark compares:
1. **VL Tokens:** The number of tokens required to represent the logic in Vibe Language.
2. **Python Tokens:** The number of tokens required to represent the *same* logic in Python (generated by the VL compiler).

**Savings Calculation:**
`Token Reduction = 1 - (VL_Tokens / Python_Tokens)`

High token reduction percentages directly translate to lower API costs and faster generation times when using LLMs.

## Current Results (January 30, 2026)

**Benchmark Suite Results** (13 test cases):
- **Average Token Efficiency: 23.8%**
- **Total: 262 VL tokens vs 344 Python tokens**

**Best Performers:**
- Data Pipeline: 45.2% savings
- Simple Function: 41.7% savings
- Boolean Logic: 34.4% savings
- Conditional Return: 34.4% savings
- Complex Logic: 25.5% savings
- Recursion: 25.6% savings

**Features Tested:**
- ✅ Infix operators (+, -, *, /, &&, ||, !)
- ✅ Direct function calls with @
- ✅ Pipeline from expressions
- ✅ Recursion with @
- ✅ Data transformations
- ✅ Complex conditionals

**Note:** These results use modern VL syntax with infix operators. The older `op:` prefix syntax showed lower efficiency and is no longer recommended.
