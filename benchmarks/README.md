# VL Benchmarking Suite

This directory contains tools to measure and verify the token efficiency of Vibe Language compared to Python.

## Prerequisites

To run the benchmarks effectively and get accurate token counts (matching GPT-4/Claude), you need the `tiktoken` library.

```bash
pip install tiktoken
```

## Running Benchmarks

Run the benchmark script from the root of the repository:

```bash
python benchmarks/benchmark_suite.py
```

## Methodology

The benchmark compares:
1. **VL Tokens:** The number of tokens required to represent the logic in Vibe Language.
2. **Python Tokens:** The number of tokens required to represent the *same* logic in Python (generated by the VL compiler).

**Savings Calculation:**
`Token Reduction = 1 - (VL_Tokens / Python_Tokens)`

High token reduction percentages directly translate to lower API costs and faster generation times when using LLMs.

## Current Results (January 30, 2026)

**Benchmark Suite Results** (13 test cases):
- **Average Token Efficiency: 41.3%**
- **Total: 240 VL tokens vs 409 Python tokens**

**Best Performers:**
- Data Pipeline: 84.8% savings
- Simple Function: 39.1% savings
- Boolean Logic: 30.0% savings
- Conditional Return: 30.0% savings
- Loop with Accumulator: 30.0% savings
- Multi-step Calculation: 30.0% savings

**Features Tested:**
- ✅ Infix operators (+, -, *, /, &&, ||, !)
- ✅ Direct function calls with @
- ✅ Pipeline from expressions
- ✅ Recursion with @
- ✅ Implicit variables (x=5)
- ✅ Compound operators (+=, -=, *=, /=)
- ✅ Range shorthand (0..10)
- ✅ Data transformations
- ✅ Complex conditionals

**Note:** These results use modern VL syntax with infix operators. The older `op:` prefix syntax showed lower efficiency and is no longer recommended.
